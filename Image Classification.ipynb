{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "96d6f2a1-4cc3-4840-94c8-6b4807275de6"
            },
            "source": "<h1><h1>Image Classification with PyTorch </h1>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "e9f96647-03e0-4d4e-a4aa-5b65647b406d"
            },
            "source": "In this project pre-trained models have been used to classify between the negative and positive samples of a dataset. This dataset comprises of images of tiles and the objective is to identify the tiles that have cracks and hence should not be used. The particular pre-trained model used here is resnet18. The following three main tasks have been accomplished using PyTorch:\n\n<ul>\n<li>Changed the output layer</li>\n<li>Trained the model</li> \n<li>Identified  several  misclassified samples</li> \n </ul>\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "3ab5fb36-e725-4078-84c6-61b4e4f321a5"
            },
            "source": "<h2>Table of Contents</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "a625a5b9-6353-4889-8910-4fe50cc18067"
            },
            "source": "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<ul>\n    <li><a> Download Data</a></li>\n    <li><a> Imports and Auxiliary Functions </a></li>\n    <li><a> Dataset Class</a></li>\n    <li><a> Prepare a pre-trained resnet18 model</a></li>\n    <li><a> Train the Model</a></li>\n    <li><a> Missclassified Samples</a></li>\n</ul>\n\n\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "b1bc42c1-8581-4803-a1de-37fd3f106e78"
            },
            "source": "<h2 id=\"download_data\">Download Data</h2>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "73b8bf88-3151-4861-be14-8e8f6827a7a5"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2022-06-23 13:48:00--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2598656062 (2.4G) [application/zip]\nSaving to: \u2018Positive_tensors.zip\u2019\n\nPositive_tensors.zi 100%[===================>]   2.42G  30.3MB/s    in 81s     \n\n2022-06-23 13:49:21 (30.7 MB/s) - \u2018Positive_tensors.zip\u2019 saved [2598656062/2598656062]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "0eb9901a-b9d9-48ef-aa3b-888d771a68ea"
            },
            "outputs": [],
            "source": "!unzip -q Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "429ad6f6-1d35-41c1-9c13-e4318af5f8e7"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2022-06-23 13:50:36--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2111408108 (2.0G) [application/zip]\nSaving to: \u2018Negative_tensors.zip\u2019\n\nNegative_tensors.zi 100%[===================>]   1.97G  27.8MB/s    in 69s     \n\n2022-06-23 13:51:46 (29.0 MB/s) - \u2018Negative_tensors.zip\u2019 saved [2111408108/2111408108]\n\n"
                }
            ],
            "source": "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n!unzip -q Negative_tensors.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2d544afa-2939-48af-9a13-a61f0fdaccd3"
            },
            "source": "Install torchvision:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "2deab7fa-435d-41c6-b2dd-7190e1aee6a4"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: torchvision in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (0.11.2)\r\nRequirement already satisfied: numpy in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from torchvision) (1.20.3)\r\nRequirement already satisfied: torch in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from torchvision) (1.10.1)\r\nRequirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from torchvision) (8.4.0)\r\nRequirement already satisfied: typing_extensions in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from torch->torchvision) (3.7.4.3)\r\n"
                }
            ],
            "source": "!pip install torchvision"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "id": "9aa8897a-7d4a-43a2-b2b0-516dc5f19d33"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/home/wsuser/work\r\n"
                }
            ],
            "source": "!pwd"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "id": "88ed8114-64bf-40f6-848e-8a42198080ac"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/usr/bin/sh: line 0: cd: /home/dsxuser/work/Positive_tensors: No such file or directory\r\n"
                }
            ],
            "source": "! cd /home/dsxuser/work/Positive_tensors\n"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "id": "0f89e8ad-d2d8-4d81-85f5-bae92cc2906f"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Negative_tensors  Negative_tensors.zip\tPositive_tensors  Positive_tensors.zip\r\n"
                }
            ],
            "source": "!ls"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "id": "ab9660e8-bfaa-4a06-80a3-85617b99389c"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/home/wsuser/work\r\n"
                }
            ],
            "source": "!pwd"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "id": "377ebeeb-8dc4-4d72-94c4-9c44620db1ce"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Negative_tensors  Negative_tensors.zip\tPositive_tensors  Positive_tensors.zip\r\n"
                }
            ],
            "source": "!cd /home/wsuser/work/Positive_tensors\n!ls"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "id": "b401aa7d-5947-46ad-8881-c57fc0589a41"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Negative_tensors  Negative_tensors.zip\tPositive_tensors  Positive_tensors.zip\r\n"
                }
            ],
            "source": "!ls"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "e444974f-5d6e-4692-8a9a-1127adcb8ce2"
            },
            "source": "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5b7db94b-8349-4b2e-9ff3-1a9a3c0ec030"
            },
            "source": "The following are the libraries required. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "id": "b43c1e58-b9b7-4f99-a330-ca00444922db"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "<torch._C.Generator at 0x7fc7784dbd50>"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# These are the libraries will be used for this lab.\nimport torchvision.models as models\nfrom PIL import Image\nimport pandas\nfrom torchvision import transforms\nimport torch.nn as nn\nimport time\nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport h5py\nimport os\nimport glob\ntorch.manual_seed(0)"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "id": "97ed31bf-ff50-4ada-808d-344a63f51571"
            },
            "outputs": [],
            "source": "from matplotlib.pyplot import imshow\nimport matplotlib.pylab as plt\nfrom PIL import Image\nimport pandas as pd\nimport os"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "abfa085a-c48f-4973-a763-8f13389a35c9"
            },
            "source": "<!--Empty Space for separating topics-->\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "6d598e73-b670-4818-a16d-2c2065f6daac"
            },
            "source": "<h2 id=\"data_class\">Dataset Class</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "c0605e45-4e4e-4c3b-a3a3-4eed31064f09"
            },
            "source": "Build the dataset class \n"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "id": "d826df5a-976e-4927-a372-3e74aa4355cb"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "# Create your own dataset object\n\nclass Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"/home/dsxuser/work\"\n        positive=\"Positive_tensors\"\n        negative='Negative_tensors'\n\n        positive_file_path=os.path.join(directory,positive)\n        positive_file_path='/home/wsuser/work/Positive_tensors'\n        print(positive_file_path)\n        negative_file_path=os.path.join(directory,negative)\n        negative_file_path='/home/wsuser/work/Negative_tensors'\n        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)     \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n               \n        image=torch.load(self.all_files[idx])\n        y=self.Y[idx]\n                  \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y\n    \nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1d3d2c38-db6f-4c25-9064-ba49d3566adb"
            },
            "source": "Create two dataset objects, one for the training data and one for the validation data.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "id": "b10b2a7b-4c2e-4c11-9683-75e56da802c6"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/home/wsuser/work/Positive_tensors\n/home/wsuser/work/Positive_tensors\ndone\n"
                }
            ],
            "source": "train_dataset = Dataset(train=True)\nvalidation_dataset = Dataset(train=False)\nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "f63ba060-2973-43c7-9dfb-4e274b125950"
            },
            "source": "<h2>Prepare a pre-trained resnet18 model :</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "70da9f83-0df0-4c32-9eee-62796662a8c4"
            },
            "source": "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "34b6e436-a982-4602-903c-c2a01037f3a1"
            },
            "outputs": [],
            "source": "# Step 1: Load the pre-trained model resnet18\n\nmodel=models.resnet18(pretrained=True)\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ncomposed = transforms.Compose([transforms.Resize(224),\n                               transforms.ToTensor(),\n                               transforms.Normalize(mean, std)])"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "44122091-43f3-4d19-9f0b-9739b9291753"
            },
            "source": "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "id": "f7b68e4f-a8d8-42fb-83f1-70e79d790910"
            },
            "outputs": [],
            "source": "# Step 2: Set the parameter cannot be trained for the pre-trained model\n\nfor param in model.parameters():\n    param.requires_grad=False"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "51ef538a-a427-42f7-af59-02abdbfbb57f"
            },
            "source": "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "517e1d74-426d-481f-b8b0-b930ab786ccd"
            },
            "source": "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code>  the last hidden layer has 512 neurons.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "id": "32595ae5-de2d-44fa-a58e-c431018a26ef"
            },
            "outputs": [],
            "source": "model.fc=nn.Linear(512,2)"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "id": "7e72db2e-c111-4500-88fb-4740a8d16973"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n"
                }
            ],
            "source": "print(model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0ea5f55c-44b3-431a-be6b-821322f9d3a8"
            },
            "source": "<h2>Train the Model</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "81bc6caf-3ff1-4268-b035-52ed02590ee1"
            },
            "source": "<b>Step 1</b>: Create a cross entropy criterion function\n"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "id": "cffea0ec-4233-44b3-b6b5-ca1dfd751ecf"
            },
            "outputs": [],
            "source": "# Step 1: Create the loss function\n\ncriterion = nn.CrossEntropyLoss()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "39d09cb0-6e19-4826-bdc8-f0a1bf753f9b"
            },
            "source": "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "id": "52446499-ea75-43c5-8941-ac5f5e39b106"
            },
            "outputs": [],
            "source": "train_loader=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\nvalidation_loader=torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "b2f254d9-726b-4b37-8d93-bbf774203c07"
            },
            "source": "<b>Step 3</b>: Use the following optimizer to minimize the loss\n"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "id": "ce3bc89f-4460-4fa5-b83f-42ebbd55d3b9"
            },
            "outputs": [],
            "source": "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "c44e6914-1574-4128-aee3-15622947a36e"
            },
            "source": "<!--Empty Space for separating topics-->\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "bbfcb38f-1c0c-4804-b377-fe6e7ea18e92"
            },
            "source": "Calculating the accuracy on the validation data for one epoch\n"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "id": "427d4d9a-1d3d-439d-89f8-735b3485750b"
            },
            "outputs": [],
            "source": "n_epochs=1\nloss_list=[]\naccuracy_list=[]\ncorrect=0\nN_test=len(validation_dataset)\nN_train=len(train_dataset)\nstart_time = time.time()\n#n_epochs\n\nLoss=0\nstart_time = time.time()\nfor epoch in range(n_epochs):\n    for x, y in train_loader:\n\n        model.train() \n        #clear gradient \n        optimizer.zero_grad()\n        #make a prediction \n        z=model(x)\n        # calculate loss \n        loss=criterion(z,y)\n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        loss_list.append(loss.data)\n    correct=0\n    for x_test, y_test in validation_loader:\n        # set model to eval \n        model.eval()\n        #make a prediction \n        z=model(x_test)\n        #find max \n        _, yhat= torch.max(z.data,1)\n       \n        #Calculate misclassified  samples in mini-batch \n        #hint +=(yhat==y_test).sum().item()\n        correct +=(yhat==y_test).sum().item()\n   \n    accuracy=correct/N_test\n    accuracy_list.append(accuracy)\n       \n   \n    \n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "e53909a7-9f05-4755-ae07-c3c39eecdb96"
            },
            "source": "Accuracy\n"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "id": "bf7f24a7-46aa-46d4-a4df-818966c17e0b"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.9935"
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "accuracy"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "b8cde640149a459f88666727d67d93e1"
            },
            "source": "Loss Plot"
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {
                "id": "37a7b7e8-44a0-4fdf-b125-a81cb474fb62"
            },
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "plt.plot(loss_list)\nplt.xlabel(\"iteration\")\nplt.ylabel(\"loss\")\nplt.show()\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8170b786-8764-40b8-8b0b-70d5fda0c41f"
            },
            "source": "<h2>Misclassified samples</h2> \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "3677a724-f1d9-4ccf-833d-167f982c9ca2"
            },
            "source": "<b>Identifying the first four misclassified samples using the validation data:</b>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {
                "id": "2a3a08cc361848f38b0f7d1a3f639ee6"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "sample#: 2 - predicted value: 1 - actual value: 0\nsample#: 27 - predicted value: 0 - actual value: 1\nsample#: 80 - predicted value: 1 - actual value: 0\nsample#: 246 - predicted value: 1 - actual value: 0\ndone!\n"
                }
            ],
            "source": "sampleSeq=0\nN_samples=0\n\nfor x_test, y_test in validation_loader:\n    model.eval()\n    z=model(x_test)\n    _, yhat=torch.max(z.data,1)\n    \n    for i in range(len(y_test)):\n        sampleSeq += 1\n        if yhat[i] != y_test[i]:\n            print(\"sample#: %d - predicted value: %d - actual value: %d\" % (sampleSeq, yhat[i], y_test[i]))\n            N_samples += 1\n            if N_samples >= 4:\n                break\n    if N_samples >=4:\n        break\n        \nprint(\"done!\")"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}